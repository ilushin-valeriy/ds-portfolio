{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Содержание<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Подготовка\" data-toc-modified-id=\"Подготовка-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Подготовка</a></span></li><li><span><a href=\"#Обучение\" data-toc-modified-id=\"Обучение-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Обучение</a></span><ul class=\"toc-item\"><li><span><a href=\"#TF-IDF\" data-toc-modified-id=\"TF-IDF-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>TF-IDF</a></span></li><li><span><a href=\"#Tunned-BERT-+-emmbeding\" data-toc-modified-id=\"Tunned-BERT-+-emmbeding-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Tunned BERT + emmbeding</a></span></li><li><span><a href=\"#BERT-+-Fine-tunning\" data-toc-modified-id=\"BERT-+-Fine-tunning-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>BERT + Fine tunning</a></span></li><li><span><a href=\"#Использование-и-тестирование-обученной-BERT-модели\" data-toc-modified-id=\"Использование-и-тестирование-обученной-BERT-модели-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Использование и тестирование обученной BERT-модели</a></span></li></ul></li><li><span><a href=\"#Выводы\" data-toc-modified-id=\"Выводы-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Выводы</a></span></li><li><span><a href=\"#Чек-лист-проверки\" data-toc-modified-id=\"Чек-лист-проверки-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Чек-лист проверки</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проект анализа токсичности комментарие для Интернет-магазина"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интернет-магазин запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Требуется инструмент, который будет искать токсичные комментарии и отправлять их на модерацию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.9/site-packages (0.0.53)\n",
      "Requirement already satisfied: regex in /opt/conda/lib/python3.9/site-packages (from sacremoses) (2022.8.17)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.9/site-packages (from sacremoses) (1.2.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.9/site-packages (from sacremoses) (8.1.3)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.9/site-packages (from sacremoses) (1.16.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from sacremoses) (4.65.0)\n"
     ]
    }
   ],
   "source": [
    "## установка недостающих библиотек\n",
    "# %%script echo\n",
    "! pip install -q evaluate\n",
    "! pip install -q sentencepiece\n",
    "! pip install -U sacremoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.2.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0-py3-none-any.whl (13.9 MB)\n",
      "\u001B[K     |████████████████████████████████| 13.9 MB 1.3 MB/s eta 0:00:01\n",
      "\u001B[?25hRequirement already satisfied: spacy<3.3.0,>=3.2.0 in /opt/conda/lib/python3.9/site-packages (from en-core-web-sm==3.2.0) (3.2.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.10)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.7.8)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.21.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.8)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.4)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.10.1)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.6.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.25.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.8.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.1)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (49.6.0.post20210108)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.65.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.7)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.3.0)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.17)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /opt/conda/lib/python3.9/site-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.3.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.26.6)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2022.6.15)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.0.0)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.9/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.9/site-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.1.1)\n",
      "\u001B[38;5;2m✔ Download and installation successful\u001B[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "# %%script echo\n",
    "! python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.21.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ssl\n",
    "import os\n",
    "\n",
    "import evaluate\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import torch\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import notebook\n",
    "from transformers import (AutoTokenizer,\n",
    "                          AutoModelForSequenceClassification,\n",
    "                          TextClassificationPipeline,\n",
    "                          get_scheduler)\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## константы будущего обучения\n",
    "TEST_SIZE = 0.2\n",
    "RANDOM_STATE = 15071982\n",
    "CV_SIZE = 5"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    data = pd.read_csv('datasets/toxic_comments.csv')\n",
    "except:\n",
    "    data = pd.read_csv('/datasets/toxic_comments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  toxic\n",
       "0           0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1           1  D'aww! He matches this background colour I'm s...      0\n",
       "2           2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3           3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4           4  You, sir, are my hero. Any chance you remember...      0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159292 entries, 0 to 159291\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   Unnamed: 0  159292 non-null  int64 \n",
      " 1   text        159292 non-null  object\n",
      " 2   toxic       159292 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 3.6+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### удалим ненужную колонку\n",
    "data.drop(columns='Unnamed: 0', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Лемматизация\n",
    "spacy_lemmatizer = spacy.load(\"en_core_web_sm\")\n",
    "def do_lemmatize(text):\n",
    "    parsed_text = spacy_lemmatizer(text)\n",
    "    return \" \".join([w.lemma_ for w in parsed_text if not w.is_punct and not w.like_num and not w.is_space])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "nltk.download('stopwords')\n",
    "stopwords = list(nltk_stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test = train_test_split(data, test_size=TEST_SIZE, stratify=data['toxic'], random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tf_idf_pipeline():\n",
    "    return Pipeline([\n",
    "        (\"vect\", TfidfVectorizer(stop_words = stopwords)),\n",
    "        (\"model\", LogisticRegression(max_iter=1000))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%script echo\n",
    "tf_idf_pipeline = create_tf_idf_pipeline()\n",
    "tf_idf_pipeline.fit(data_train['text_lemm'],data_train['toxic'])\n",
    "\n",
    "predictions_test = tf_idf_pipeline.predict(data_test['text_lemm'])\n",
    "f1_score_test = f1_score(data_test['toxic'], predictions_test)\n",
    "print(f'f1-score на тесте: {f1_score_test}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%script echo\n",
    "\n",
    "parameters = {'model__C': np.linspace(0.0001, 100, 20)}\n",
    "grid_search = GridSearchCV(create_tf_idf_pipeline(),\n",
    "    parameters, scoring='f1', cv=CV_SIZE, n_jobs=-1)\n",
    "\n",
    "grid_search.fit(data_train['text_lemm'], data_train['toxic'])\n",
    "\n",
    "print('Лучшие параметры: ', grid_search.best_params_)\n",
    "print(f'f1-score на cross-валидации: {grid_search.best_score_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%script echo\n",
    "predictions_test = grid_search.predict(data_test['text_lemm'])\n",
    "f1_score_test = f1_score(data_test['toxic'], predictions_test)\n",
    "print(f'f1-score на тесте: {f1_score_test}')\n",
    "\n",
    "\n",
    "### Получено в домашних условиях\n",
    "# Лучшие параметры:  {'model__C': 21.05271052631579}\n",
    "# f1-score на cross-валидации: 0.7757574153726411\n",
    "# f1-score на тесте: 0.7732558139534884"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Выводы\n",
    "Модель на основе TF-IDF и LogisticRegression с гиперпараметром С=21.05, показали требуемую точность на тестовой выборке с f1-score=0.77"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tunned BERT + emmbeding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим готовую модель и токенезатор с huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/user/.cache/torch/hub/huggingface_pytorch-transformers_main\n",
      "Using cache found in /Users/user/.cache/torch/hub/huggingface_pytorch-transformers_main\n",
      "Some weights of the model checkpoint at martin-ha/toxic-comment-model were not used when initializing DistilBertModel: ['classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_name = 'martin-ha/toxic-comment-model'\n",
    "# model_name = 'bert-large-uncased'\n",
    "# model_name = 'bert-base-uncased'\n",
    "# model_name = 'distilbert-base-uncased'\n",
    "# model_name = 'unitary/toxic-bert'\n",
    "\n",
    "tokenizer = torch.hub.load('huggingface/pytorch-transformers',\n",
    "                           'tokenizer',\n",
    "                           model_name,\n",
    "                           do_lower_case=True)\n",
    "model = torch.hub.load('huggingface/pytorch-transformers', 'model', model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выполним кодирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.8965\n",
       "1    0.1035\n",
       "Name: toxic, dtype: float64"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BERT_DATA_SIZE = 10000\n",
    "df_bert = data.sample(BERT_DATA_SIZE, random_state=RANDOM_STATE)\n",
    "df_bert['toxic'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = df_bert['text'].apply(\n",
    "    lambda x: tokenizer.encode(x, add_special_tokens=True, truncation=True, max_length=512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max lenght = 512\n"
     ]
    }
   ],
   "source": [
    "max_len = 0\n",
    "for i in tokenized.values:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "\n",
    "print(f'max lenght = {max_len}')\n",
    "\n",
    "padded = np.array([i + [0] * (max_len - len(i)) for i in tokenized.values])\n",
    "attention_mask = np.where(padded != 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "pycharm": {
     "name": "#%%script echo\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%script echo\n",
    "batch_size = 100\n",
    "embeddings = []\n",
    "for i in notebook.tqdm(range(padded.shape[0] // batch_size)):\n",
    "    batch = torch.LongTensor(padded[batch_size * i:batch_size * (i + 1)])\n",
    "    attention_mask_batch = torch.LongTensor(attention_mask[batch_size * i:batch_size * (i + 1)])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        batch_embeddings = model(batch, attention_mask=attention_mask_batch)\n",
    "\n",
    "    embeddings.append(batch_embeddings[0][:, 0, :].numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%script echo\n",
    "features = np.concatenate(embeddings)\n",
    "features_train, features_test, target_train, target_test = train_test_split(features,\n",
    "                                                                            df_bert['toxic'],\n",
    "                                                                            test_size=TEST_SIZE,\n",
    "                                                                            stratify=df_bert['toxic'])\n",
    "\n",
    "parameters = {'C': np.linspace(0.0001, 100, 20)}\n",
    "grid_search = GridSearchCV(LogisticRegression(max_iter=10000), parameters, scoring='f1', cv=CV_SIZE, n_jobs=-1)\n",
    "grid_search.fit(features_train, target_train)\n",
    "\n",
    "predictions_train = grid_search.predict(features_train)\n",
    "print('Лучшие параметры: ', grid_search.best_params_)\n",
    "print(f'f1-score на cross-валидации: {grid_search.best_score_}')\n",
    "\n",
    "predictions_test = grid_search.predict(features_test)\n",
    "print(f'f1-score на тесте: {f1_score(target_test, predictions_test)}')\n",
    "\n",
    "print()\n",
    "\n",
    "### Получено в домашних условиях\n",
    "# bert-large-uncased (datasize = 400)\n",
    "# Лучшие параметры:  {'C': 10.526405263157894}\n",
    "# f1-score на cross-валидации: 0.5938375350140056\n",
    "# f1-score на тесте: 0.28571428571428575\n",
    "\n",
    "# bert-base-uncased (datasize = 400)\n",
    "# Лучшие параметры:  {'C': 31.579015789473683}\n",
    "# f1-score на cross-валидации: 0.5004195804195805\n",
    "# f1-score на тесте: 0.625\n",
    "\n",
    "# distilbert-base-uncased (datasize = 400)\n",
    "# Лучшие параметры:  {'C': 5.263252631578947}\n",
    "# f1-score на cross-валидации: 0.6257664884135472\n",
    "# f1-score на тесте: 0.7499999999999999\n",
    "\n",
    "# martin-ha/toxic-comment-model (datasize = 400)\n",
    "# Лучшие параметры:  {'C': 5.263252631578947}\n",
    "# f1-score на cross-валидации: 0.6229181929181928\n",
    "# f1-score на тесте: 0.888888888888889\n",
    "\n",
    "# martin-ha/toxic-comment-model (datasize = 10000)\n",
    "# Лучшие параметры:  {'C': 89.4736947368421}\n",
    "# f1-score на cross-валидации: 0.7406059478496869\n",
    "# f1-score на тесте: 0.7206703910614525"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С использованием готовой модели для распознования токсичных комментариев у нас получилось добиться f1-score 0.7206"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT + Fine tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape train (320, 2)\n",
      "shape test (80, 2)\n"
     ]
    }
   ],
   "source": [
    "BERT_DATA_SIZE = 400\n",
    "\n",
    "LOADER_BATCH_SIZE = 8\n",
    "NUM_EPOCHS = 1\n",
    "\n",
    "df, df_save = train_test_split(data, test_size=0.2, stratify=data['toxic'], random_state=RANDOM_STATE)\n",
    "\n",
    "if BERT_DATA_SIZE >= 100_000:\n",
    "    df = df.copy()\n",
    "else:\n",
    "    df = df.sample(BERT_DATA_SIZE, random_state=RANDOM_STATE)\n",
    "\n",
    "data_train, data_test = train_test_split(df, test_size=TEST_SIZE, stratify=df['toxic'], random_state=RANDOM_STATE)\n",
    "print(f'shape train {data_train.shape}')\n",
    "print(f'shape test {data_test.shape}')\n",
    "\n",
    "data_train = Dataset.from_pandas(data_train)\n",
    "data_test = Dataset.from_pandas(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True)\n",
    "\n",
    "\n",
    "def ds_preproc(ds):\n",
    "    ds = ds.map(tokenize_function)\n",
    "    ds = ds.remove_columns(['text', '__index_level_0__'])\n",
    "    ds = ds.rename_column('toxic', 'labels')\n",
    "    ds.set_format('torch')\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.009708166122436523,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Map",
       "rate": null,
       "total": 320,
       "unit": " examples",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a073a64c546949bc9975d95f418bd385",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/320 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.010919809341430664,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Map",
       "rate": null,
       "total": 80,
       "unit": " examples",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a280bca3f7494950a81418f5eedd6251",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/80 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "tokenized_train = ds_preproc(data_train)\n",
    "tokenized_test = ds_preproc(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(tokenized_train, shuffle=True, batch_size=LOADER_BATCH_SIZE)\n",
    "test_dataloader = DataLoader(tokenized_test, batch_size=LOADER_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%script echo\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-6)\n",
    "\n",
    "num_training_steps = NUM_EPOCHS * len(train_dataloader)\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    name='linear',\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps)\n",
    "\n",
    "device = 'cpu'\n",
    "model.to(device)\n",
    "\n",
    "# Выполняем цикл...\n",
    "for epoch in notebook.tqdm(range(NUM_EPOCHS)):\n",
    "\n",
    "    #... обучения\n",
    "    model.train()\n",
    "    for batch in notebook.tqdm(train_dataloader, leave=False):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    #... оценки\n",
    "    metric = evaluate.load('f1')\n",
    "\n",
    "    model.eval()\n",
    "    for batch in notebook.tqdm(test_dataloader, leave=False):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        metric.add_batch(predictions=predictions, references=batch['labels'])\n",
    "\n",
    "    print(f'epoch {epoch} -', metric.compute())\n",
    "\n",
    "### получено в домашних условиях\n",
    "# BERT_DATA_SIZE = 4000\n",
    "# LOADER_BATCH_SIZE = 100\n",
    "# NUM_EPOCHS = 1\n",
    "# 2:51:00\n",
    "# epoch 0 - {'f1': 0.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%script echo\n",
    "# Сохраняем модель\n",
    "save_directory = f'./models/my_pretrained_toxic_{NUM_EPOCHS}_{LOADER_BATCH_SIZE}_{BERT_DATA_SIZE}'\n",
    "model.save_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Использование и тестирование обученной BERT-модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.sample(10000, random_state=RANDOM_STATE)\n",
    "model_path = \"martin-ha/toxic-comment-model\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "toxic_pipeline =  TextClassificationPipeline(model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_toxic_flag(text):\n",
    "    result = toxic_pipeline(text, padding=True, truncation=True)\n",
    "    if result[0]['label'] == 'toxic':\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for \"you are Mother fucker\" toxic flag is 1\n",
      "for \"you are lucky guy\" toxic flag is 0\n"
     ]
    }
   ],
   "source": [
    "## пример работы готовой модели\n",
    "for text in ['you are Mother fucker', 'you are lucky guy']:\n",
    "    print(f'for \"{text}\" toxic flag is {get_toxic_flag(text)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%script echo\n",
    "%time df['toxic_flag_prediction'] = df['text'].apply(get_toxic_flag)\n",
    "\n",
    "### получено в домашних условниях\n",
    "# выборка 10000\n",
    "# CPU times: user 53min 35s, sys: 2min 4s, total: 55min 40s\n",
    "# Wall time: 7min 8s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%script echo\n",
    "df['toxic_flag_prediction'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%script echo\n",
    "f1_score_test = f1_score(df['toxic'],df['toxic_flag_prediction'])\n",
    "print(f'f1-score : {f1_score_test}')\n",
    "\n",
    "## получено в домашних условиях\n",
    "# выборка 10000\n",
    "# f1-score : 0.6963048498845266"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В рамках проведенного исследования мы обучили модель распознавания токсичных комментариев на основе TF-IDF и LogisticRegression. С ипользованием гиперпараметра С=21.05, модель достигла на тесовой выборке метрики f1-score=0.77, что постановке задачи.\n",
    "\n",
    "Так же было проведены попытки построить модель на основе предобученной модели BERT в 2х вариантах:\n",
    "* построение emmbeding'ов на основе готовой модели [martin-ha/toxic-comment-model](https://huggingface.co/martin-ha/toxic-comment-model) + логистическая регрессия - f1-score на тесте: 0.72\n",
    "* дообучение bert-base-uncased для бинарной классификаци, к сожалению, доступное оборудование не позволяет дообучать данную модель с требуемой точностью за разумное время - поэтому результаты не приведены, код исследования оставлен для справки\n"
   ]
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 3226,
    "start_time": "2023-03-06T18:46:40.342Z"
   },
   {
    "duration": 0,
    "start_time": "2023-03-06T18:46:43.570Z"
   },
   {
    "duration": 0,
    "start_time": "2023-03-06T18:46:43.570Z"
   },
   {
    "duration": 0,
    "start_time": "2023-03-06T18:46:43.571Z"
   },
   {
    "duration": 0,
    "start_time": "2023-03-06T18:46:43.572Z"
   },
   {
    "duration": 0,
    "start_time": "2023-03-06T18:46:43.572Z"
   },
   {
    "duration": 1,
    "start_time": "2023-03-06T18:46:43.577Z"
   },
   {
    "duration": 0,
    "start_time": "2023-03-06T18:46:43.578Z"
   },
   {
    "duration": 116,
    "start_time": "2023-03-06T18:50:48.631Z"
   },
   {
    "duration": 125,
    "start_time": "2023-03-06T18:51:14.757Z"
   },
   {
    "duration": 25541,
    "start_time": "2023-03-06T18:51:39.410Z"
   },
   {
    "duration": 16195,
    "start_time": "2023-03-06T18:55:06.614Z"
   },
   {
    "duration": 7633,
    "start_time": "2023-03-06T19:25:37.440Z"
   },
   {
    "duration": 7,
    "start_time": "2023-03-06T19:25:45.076Z"
   },
   {
    "duration": 1823,
    "start_time": "2023-03-06T19:25:45.084Z"
   },
   {
    "duration": 15,
    "start_time": "2023-03-06T19:25:46.910Z"
   },
   {
    "duration": 55,
    "start_time": "2023-03-06T19:25:46.927Z"
   },
   {
    "duration": 11,
    "start_time": "2023-03-06T19:25:46.985Z"
   },
   {
    "duration": 177,
    "start_time": "2023-03-06T19:25:46.998Z"
   },
   {
    "duration": 50,
    "start_time": "2023-03-06T19:25:47.177Z"
   },
   {
    "duration": 8,
    "start_time": "2023-03-06T19:25:47.229Z"
   },
   {
    "duration": 22,
    "start_time": "2023-03-08T11:59:21.677Z"
   },
   {
    "duration": 8,
    "start_time": "2023-03-08T11:59:21.701Z"
   },
   {
    "duration": 61,
    "start_time": "2023-03-08T11:59:21.711Z"
   },
   {
    "duration": 1,
    "start_time": "2023-03-08T11:59:21.774Z"
   },
   {
    "duration": 0,
    "start_time": "2023-03-08T11:59:21.776Z"
   },
   {
    "duration": 0,
    "start_time": "2023-03-08T11:59:21.778Z"
   },
   {
    "duration": 0,
    "start_time": "2023-03-08T11:59:21.779Z"
   },
   {
    "duration": 0,
    "start_time": "2023-03-08T11:59:21.780Z"
   },
   {
    "duration": 0,
    "start_time": "2023-03-08T11:59:21.781Z"
   },
   {
    "duration": 0,
    "start_time": "2023-03-08T11:59:21.782Z"
   },
   {
    "duration": 46938,
    "start_time": "2023-03-08T11:59:55.778Z"
   },
   {
    "duration": 10473,
    "start_time": "2023-03-08T12:00:42.719Z"
   },
   {
    "duration": 14718,
    "start_time": "2023-03-08T12:00:53.194Z"
   },
   {
    "duration": 1515,
    "start_time": "2023-03-08T12:01:07.914Z"
   },
   {
    "duration": 12,
    "start_time": "2023-03-08T12:01:09.432Z"
   },
   {
    "duration": 59,
    "start_time": "2023-03-08T12:01:09.445Z"
   },
   {
    "duration": 11,
    "start_time": "2023-03-08T12:01:09.506Z"
   },
   {
    "duration": 15,
    "start_time": "2023-03-08T12:01:09.520Z"
   },
   {
    "duration": 933,
    "start_time": "2023-03-08T12:01:09.537Z"
   },
   {
    "duration": 169,
    "start_time": "2023-03-08T12:01:10.472Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "302.391px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
